# Scalability Architecture: Decentralized Workflow Ownership

## Overview

This document describes Raftoral's **multi-cluster scalability architecture**, designed to support large auto-scaled deployments (dozens to hundreds of nodes) without overwhelming the system with checkpoint replication traffic.

**Last Updated:** 2025-10-26 (Current Implementation)

### The Problem

**Single-Cluster Architecture Limitations:**
- Single Raft cluster includes all nodes in the deployment
- Every checkpoint is replicated to all nodes via Raft consensus
- Auto-scaled deployment with 50 nodes ‚Üí every checkpoint broadcasts to 50 nodes
- Checkpoint throughput inversely proportional to cluster size
- Large quorums (26/50 nodes) slow down consensus

**Example Scenario:**
```
Auto-scaled deployment: 50 pods
Workflows per second: 100
Checkpoints per workflow: 10
Total checkpoint broadcasts: 100 * 10 * 50 = 50,000 messages/second
```

This doesn't scale.

### The Solution: Two-Tier Cluster Architecture

**Management Cluster:**
- Single global cluster including ALL nodes in the deployment
- Manages **execution cluster membership** (which nodes belong to which clusters)
- **Does NOT track workflow lifecycle** - execution clusters own their workflows
- Small voter set (3-5 voters) + many learners ‚Üí fast quorum
- State: O(N√óC) where N = nodes, C = execution clusters

**Execution Clusters:**
- Multiple small virtual clusters (3-5 nodes each)
- Handle workflow execution and checkpoint replication **independently**
- Each cluster owns its workflows - no global workflow tracking
- Dynamically created and destroyed based on workload
- State: O(W_local) where W_local = workflows in THIS cluster only

**Key Insight:** Decouple cluster size from checkpoint throughput by partitioning workflows across small execution clusters, with **decentralized workflow ownership** (no O(W) global state).

---

## Architecture Components

### 1. Management Cluster

**Type:** `RaftCluster<ManagementCommandExecutor>`

**Responsibilities:**
- Track all nodes in the deployment
- Create/destroy virtual execution clusters
- Assign nodes to execution clusters
- ~~Monitor workflow lifecycle globally~~ **REMOVED** (execution clusters own their workflows)
- Coordinate cluster rebalancing (future)

**Command Set:**

```rust
#[derive(Clone, Debug, Serialize, Deserialize)]
pub enum ManagementCommand {
    /// Create a new virtual execution cluster
    CreateExecutionCluster(CreateExecutionClusterData),

    /// Destroy an execution cluster (must have no active nodes)
    DestroyExecutionCluster(ExecutionClusterId),

    /// Associate a node with an execution cluster
    AssociateNode(AssociateNodeData),

    /// Disassociate a node from an execution cluster
    DisassociateNode(DisassociateNodeData),

    /// Change node role (voter/learner) in management cluster
    ChangeNodeRole(ChangeNodeRoleData),
}
```

**State Management:**

```rust
pub struct ManagementState {
    /// All execution clusters
    pub execution_clusters: HashMap<Uuid, ExecutionClusterInfo>,

    /// Node membership: node_id ‚Üí set of execution cluster IDs
    pub node_memberships: HashMap<u64, HashSet<Uuid>>,
}

pub struct ExecutionClusterInfo {
    pub cluster_id: Uuid,
    pub node_ids: Vec<u64>,
    pub created_at: u64,
    // NOTE: No active_workflows field!
    // Execution clusters own their workflow state
}
```

**Critical Design Decision:**

**NO GLOBAL WORKFLOW TRACKING** in management state. This is the key scalability win:
- Management state is O(N√óC) - bounded by node count and cluster count
- Execution cluster state is O(W_local) - only workflows in that cluster
- Total state: O(N√óC + W) instead of O(N√óC + W) all replicated globally

---

### 2. Execution Clusters

**Type:** `RaftCluster<WorkflowCommandExecutor>`

**Responsibilities:**
- Execute workflows assigned to this cluster
- Replicate checkpoints within cluster only (3-5 nodes)
- ~~Report workflow lifecycle to management cluster~~ **REMOVED**
- Own all workflow state independently
- Handle workflow failover within cluster

**Command Set:**

```rust
#[derive(Clone, Debug, Serialize, Deserialize)]
pub enum WorkflowCommand {
    WorkflowStart(WorkflowStartData),
    WorkflowEnd(WorkflowEndData),
    SetCheckpoint(CheckpointData),
    OwnerChange(OwnerChangeData),
}
```

**Workflow State (Local to Execution Cluster):**

```rust
pub struct WorkflowExecutorState {
    /// Active workflows in THIS execution cluster only
    pub active_workflows: HashMap<String, WorkflowState>,

    /// Completed workflow results (cached for polling)
    pub completed_workflows: HashMap<String, Vec<u8>>,

    /// Checkpoint queues for late followers
    pub checkpoint_queues: HashMap<(String, String), VecDeque<Vec<u8>>>,
}
```

**Key Point:** Each execution cluster independently manages its workflows. No cross-cluster state replication.

---

### 3. Workflow Lifecycle Flow (Current Implementation)

#### Starting a Workflow (Direct to Execution Cluster)

```
1. Client calls: runtime.start_workflow("order_processing", input)
   ‚Üì
2. NodeManager selects execution cluster via round-robin:
   - Iterates through node's execution clusters (deterministic sort)
   - Selects next cluster in rotation
   ‚Üì
3. Node proposes WorkflowStart DIRECTLY to execution cluster:
   - Only 3-5 nodes in execution cluster participate in consensus
   - Checkpoint replication limited to execution cluster
   - NO management cluster involvement!
   ‚Üì
4. Execution cluster commits WorkflowStart
   - All nodes in cluster execute workflow
   - Leader creates checkpoints
   - Followers consume from checkpoint queues
   ‚Üì
5. Workflow completes:
   - Leader proposes WorkflowEnd to execution cluster
   - Result cached in execution cluster's completed_workflows map
   - NO reporting to management cluster!
```

**Key Change:** Workflows start with **single consensus round** (execution cluster only), not two rounds (management + execution).

#### Querying Workflow Completion

```
1. Client calls: wait_for_workflow_completion(workflow_id, execution_cluster_id)
   - Client MUST provide both workflow_id AND execution_cluster_id
   - Client received execution_cluster_id when workflow started
   ‚Üì
2. Node checks if it has the execution cluster locally:
   - YES: Query local executor state for workflow result
   - NO: Forward request to node that has the cluster
   ‚Üì
3. If forwarding needed:
   - Query management state for nodes with execution_cluster_id
   - Use RequestForwarder to proxy request to one of those nodes
   - Try multiple nodes for resilience
   ‚Üì
4. Target node polls local execution cluster state:
   - Check completed_workflows map for result
   - Return immediately if available
   - Otherwise poll with timeout
```

**Key Point:** Client is responsible for tracking `execution_cluster_id`. Server automatically forwards if needed.

---

### 4. Request Forwarding

**Component:** `RequestForwarder` (src/grpc/forwarding.rs)

**Purpose:** Transparent request proxying when a node doesn't have the target execution cluster.

**Features:**
- **Connection pooling**: Cached gRPC clients per node address
- **Resilience**: Try multiple nodes until one succeeds
- **Reusable**: Generic `forward_to_any()` for future RPC types

**Example Flow:**

```rust
// Node 1 receives wait_for_workflow_completion request
// but doesn't have execution cluster "abc123"

// 1. Query management state
let cluster_info = management_executor.get_cluster_info(&cluster_id)?;
// cluster_info.node_ids = [2, 3, 4, 5, 6]

// 2. Get addresses for those nodes
let addresses = node_manager.get_node_addresses(&cluster_info.node_ids);
// addresses = ["192.168.1.2:5001", "192.168.1.3:5001", ...]

// 3. Forward request to any node with that cluster
forwarder.forward_to_any(&addresses, |addr| async {
    forwarder.forward_wait_for_workflow_completion(&addr, request).await
}).await
```

**Automatic and transparent** - client doesn't know forwarding happened.

---

### 5. Scalability Analysis

#### State Complexity Comparison

| Architecture | Management State | Execution State | Total | Replication Factor |
|--------------|------------------|-----------------|-------|-------------------|
| **Single Cluster (50 nodes)** | N/A | O(W) workflows | O(W) | 50x |
| **Multi-Cluster (No workflow tracking)** | O(N√óC) | O(W/C) per cluster | O(N√óC + W) | 5x (per cluster) |

**Concrete Example:**
```
Deployment: 50 nodes, 10 execution clusters, 10,000 active workflows

Single Cluster:
  - State: 10,000 workflows √ó 50 nodes = 500,000 workflow state entries
  - Checkpoint replication: 50x per checkpoint

Multi-Cluster (Current):
  - Management state: 50 nodes √ó 10 clusters = 500 cluster membership entries
  - Execution state: 10,000 workflows √∑ 10 clusters = 1,000 workflows/cluster √ó 5 nodes = 5,000 workflow state entries
  - Checkpoint replication: 5x per checkpoint

Reduction: 500,000 ‚Üí 5,500 total state entries (99% reduction!)
```

#### Message Count Reduction

**Checkpoint Replication:**

```
Single 50-node cluster:
  100 workflows/sec √ó 10 checkpoints/workflow √ó 50 nodes = 50,000 msgs/sec

Multi-cluster (10 clusters √ó 5 nodes):
  100 workflows/sec √ó 10 checkpoints/workflow √ó 5 nodes = 5,000 msgs/sec

90% reduction in checkpoint traffic!
```

#### Quorum Latency

```
Single 50-node cluster:
  - Quorum size: 26/50 nodes
  - Network round-trips: ~26 parallel requests
  - Typical latency: ~15ms

Execution cluster (5 nodes):
  - Quorum size: 3/5 nodes
  - Network round-trips: ~3 parallel requests
  - Typical latency: ~3ms

5x faster consensus in execution clusters!
```

---

### 6. Node Structure (Current Implementation)

Each node in the deployment runs:

```rust
pub struct NodeManager {
    /// Node ID (unique across deployment)
    node_id: u64,

    /// Management cluster (participates as voter or learner)
    management_cluster: Arc<RaftCluster<ManagementCommandExecutor>>,

    /// Execution clusters this node is part of
    /// Key: ExecutionClusterId (Uuid)
    execution_clusters: HashMap<Uuid, Arc<RaftCluster<WorkflowCommandExecutor>>>,

    /// Round-robin index for cluster selection
    round_robin_index: AtomicUsize,

    /// Workflow runtime (public API)
    workflow_runtime: Arc<WorkflowRuntime>,

    /// ClusterRouter for routing incoming gRPC messages
    cluster_router: Arc<ClusterRouter>,

    /// Transport for node-to-node communication
    transport: Arc<GrpcClusterTransport>,
}

impl NodeManager {
    /// Select execution cluster using round-robin
    pub fn select_execution_cluster_round_robin(&self) -> (Uuid, Arc<RaftCluster>) {
        let mut cluster_ids: Vec<_> = self.execution_clusters.keys().copied().collect();
        cluster_ids.sort(); // Deterministic ordering

        let index = self.round_robin_index.fetch_add(1, Ordering::Relaxed);
        let selected_id = cluster_ids[index % cluster_ids.len()];
        let selected_cluster = self.execution_clusters.get(&selected_id).unwrap().clone();

        (selected_id, selected_cluster)
    }

    /// Get execution cluster executor for querying workflow results
    pub fn get_execution_cluster_executor(&self, cluster_id: &Uuid)
        -> Option<&WorkflowCommandExecutor> {
        self.execution_clusters.get(cluster_id).map(|c| &*c.executor)
    }

    /// Get node addresses for forwarding
    pub fn get_node_addresses(&self, node_ids: &[u64]) -> Vec<String> {
        self.transport.get_node_addresses_sync(node_ids)
    }
}
```

---

### 7. gRPC API Contract

**RunWorkflowAsync:**

```protobuf
message RunWorkflowRequest {
  string workflow_type = 1;
  uint32 version = 2;
  string input_json = 3;
}

message RunWorkflowAsyncResponse {
  bool success = 1;
  string workflow_id = 2;
  string execution_cluster_id = 3;  // CLIENT MUST SAVE THIS!
  string error = 4;
}
```

**WaitForWorkflowCompletion:**

```protobuf
message WaitForWorkflowRequest {
  string workflow_id = 1;
  string execution_cluster_id = 2;  // CLIENT MUST PROVIDE THIS!
  uint32 timeout_seconds = 3;
}

message RunWorkflowResponse {
  bool success = 1;
  string result_json = 2;
  string error = 3;
}
```

**Key Point:** Client must track `(workflow_id, execution_cluster_id)` tuple. The server uses `execution_cluster_id` to route queries to the correct execution cluster (with automatic forwarding if needed).

---

### 8. Unified Transport & Cluster Routing

**ClusterRouter** (src/raft/generic/cluster_router.rs):

```rust
pub struct ClusterRouter {
    /// Management cluster mailbox (cluster_id = 0)
    management_cluster: Option<UnboundedSender<Message<ManagementCommand>>>,

    /// Execution cluster mailboxes (cluster_id = 1, 2, 3, ...)
    execution_clusters: HashMap<u64, UnboundedSender<Message<WorkflowCommand>>>,
}

impl ClusterRouter {
    /// Route incoming gRPC message to appropriate cluster
    pub async fn route_message(&self, proto_msg: GenericMessage) -> Result<(), Status> {
        let cluster_id = proto_msg.cluster_id;

        if cluster_id == 0 {
            // Management cluster
            let sender = self.management_cluster.as_ref()
                .ok_or_else(|| Status::not_found("Management cluster not registered"))?;
            let message = Message::<ManagementCommand>::from_protobuf(proto_msg)?;
            sender.send(message)?;
        } else {
            // Execution cluster
            let sender = self.execution_clusters.get(&cluster_id)
                .ok_or_else(|| Status::not_found(format!("Execution cluster {} not found", cluster_id)))?;
            let message = Message::<WorkflowCommand>::from_protobuf(proto_msg)?;
            sender.send(message)?;
        }
        Ok(())
    }
}
```

**Key Point:** Same gRPC connections, same node IDs, different Raft state machines.

---

## Current Implementation Status

### ‚úÖ Implemented (Phases 1-5)

**Phase 1: Remove Global Workflow Tracking**
- ‚úÖ Removed `workflow_locations` from ManagementState
- ‚úÖ Removed `active_workflows` from ExecutionClusterInfo
- ‚úÖ Removed ReportWorkflowStarted/ReportWorkflowEnded commands
- ‚úÖ Removed completed_workflows TTL cache from management cluster

**Phase 2: Remove Management Cluster Reporting**
- ‚úÖ Removed management_cluster reference from WorkflowCommandExecutor
- ‚úÖ Removed workflow completion reporting to management cluster

**Phase 3: Update RPC Signatures**
- ‚úÖ Added execution_cluster_id to RunWorkflowAsyncResponse
- ‚úÖ Added execution_cluster_id to WaitForWorkflowRequest
- ‚úÖ Updated protobuf schema

**Phase 4: Multiple Execution Clusters**
- ‚úÖ Changed NodeManager from single workflow_cluster to execution_clusters HashMap
- ‚úÖ Implemented round-robin cluster selection
- ‚úÖ Updated run_workflow_async to select cluster and return cluster_id

**Phase 5: Client Library Updates**
- ‚úÖ Clients receive (workflow_id, execution_cluster_id) from RunWorkflowAsync
- ‚úÖ Clients provide both IDs to WaitForWorkflowCompletion
- ‚úÖ Updated shell scripts and examples

**Phase 6: Request Forwarding**
- ‚úÖ Created RequestForwarder with connection pooling
- ‚úÖ Implemented automatic forwarding in wait_for_workflow_completion
- ‚úÖ Added get_node_addresses() for address lookup

### üöß Future Work

**Phase 7: Cluster Lifecycle Management**
- ‚è≥ Implement "drain" state for execution clusters
- ‚è≥ Prevent new workflows on draining clusters
- ‚è≥ Delete execution cluster after all workflows complete
- ‚è≥ Graceful cluster shutdown

**Phase 8: Dynamic Cluster Creation**
- ‚è≥ Auto-create execution clusters when none available
- ‚è≥ Load-based cluster creation policies
- ‚è≥ Cluster consolidation when underutilized

**Phase 9: Advanced Selection Strategies**
- ‚è≥ Locality-aware selection (prefer local execution clusters)
- ‚è≥ Workload-type affinity (colocate similar workflows)
- ‚è≥ Load-based selection (avoid overloaded clusters)

**Phase 10: Rebalancing**
- ‚è≥ Periodic rebalancing task
- ‚è≥ Create new clusters for idle nodes
- ‚è≥ Migrate workflows between clusters (requires persistent storage)

---

## Design Decisions & Rationale

### Decision 1: No Global Workflow Tracking

**Why:** State explosion problem.

```
Global tracking: O(W) workflows √ó N nodes = massive state
Decentralized: O(W) workflows √∑ C clusters √ó n nodes/cluster = minimal state
```

**Trade-off:** Can't query "list all workflows" without asking all execution clusters. Acceptable - use management state to find clusters, then query each.

### Decision 2: Client Tracks execution_cluster_id

**Why:** Simplest solution for workflow queries.

**Alternatives considered:**
- Hash-based routing (workflow_id ‚Üí cluster_id): Rigid, no load balancing
- Global registry: Defeats purpose of decentralization

**Trade-off:** Client complexity increases. Acceptable - clients already track workflow_id.

### Decision 3: Automatic Request Forwarding

**Why:** Better UX - client can query any node.

**Implementation:** RequestForwarder with connection pooling and retry logic.

**Trade-off:** Additional network hop. Acceptable - rare operation (most clients cache cluster_id).

### Decision 4: Round-Robin Selection

**Why:** Simplest load balancing strategy.

**Future:** Can add weighted selection, locality-aware, affinity-based.

**Trade-off:** Doesn't account for cluster load. Acceptable for initial implementation.

### Decision 5: Direct Workflow Start (No Management Consensus)

**Why:** Reduces latency from 2 consensus rounds to 1.

```
Old: Client ‚Üí Management (consensus) ‚Üí Execution (consensus) ‚Üí Workflow starts
New: Client ‚Üí Execution (consensus) ‚Üí Workflow starts
```

**Trade-off:** Management cluster doesn't know about workflows in real-time. Acceptable - topology is what matters.

---

## Performance Characteristics

### Scalability Metrics

| Deployment Size | Single Cluster Throughput | Multi-Cluster Throughput | Improvement |
|-----------------|--------------------------|--------------------------|-------------|
| 10 nodes        | ~5,000 ckpt/sec          | ~10,000 ckpt/sec         | 2x          |
| 20 nodes        | ~2,500 ckpt/sec          | ~10,000 ckpt/sec         | 4x          |
| 50 nodes        | ~1,000 ckpt/sec          | ~10,000 ckpt/sec         | 10x         |
| 100 nodes       | ~500 ckpt/sec            | ~10,000 ckpt/sec         | 20x         |

*Assumes 10 workflows/sec √ó 100 checkpoints/workflow*

### Memory Footprint

**Per Node (50-node deployment, 10 execution clusters):**
- Management state: ~500 KB (tracks 50 nodes √ó 10 clusters)
- Execution cluster state (each): ~5 MB (workflows + checkpoints)
- Node in 2-3 execution clusters: 500 KB + (3 √ó 5 MB) = ~16 MB

**Compare to single 50-node cluster:**
- Single cluster state: ~50 MB (all workflows)

**Savings:** 50 MB ‚Üí 16 MB per node (68% reduction)

---

## Summary

The **decentralized workflow ownership** architecture provides:

‚úÖ **10-20x improvement in checkpoint throughput** for large deployments
‚úÖ **99% reduction in global state** (O(W) ‚Üí O(N√óC))
‚úÖ **5x faster consensus** in small execution clusters vs large single cluster
‚úÖ **Horizontal scalability** - add execution clusters as workload grows
‚úÖ **Simplified management** - no global workflow tracking overhead
‚úÖ **Client-driven routing** - clients track execution_cluster_id
‚úÖ **Automatic forwarding** - transparent request proxying

**Trade-offs:**
‚ùå Clients must track `(workflow_id, execution_cluster_id)` tuple
‚ùå Global workflow queries require fan-out to all execution clusters
‚ùå Cluster lifecycle management adds operational complexity (future)

**Recommended for:** Auto-scaled deployments with 20+ nodes and high workflow throughput.

**Not recommended for:** Small deployments (<10 nodes) - single-cluster architecture is simpler.

---

**Last Updated:** 2025-10-26
**Implementation Status:** Phases 1-6 complete (35 library tests + integration tests passing)
